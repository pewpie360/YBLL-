{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Let's create our first Dataframe\n",
    "\n",
    "- Think of `DataFrame` as tables __\\*but\\*__ with a rich set of functionalities for data analysis and manipulation\n",
    "- It is the main data structure of the `pandas` library\n",
    "- Has tabular properties such as rows and columns including indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate a dataframe with dummy data\n",
    "df = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns=[\"A\",\"B\",\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It understands dictionaries too, altenatively...\n",
    "data = {\n",
    "    'A': [1, 4, 7],\n",
    "    'B': [2, 5, 8],\n",
    "    'C': [3, 6, 9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ending a code cell with a `DataFrame` will preview of its contents\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0, 1, 2...?\n",
    "\n",
    "- These are called *__indices__*\n",
    "- They are used to __uniquely identify a row__ and pandas automatically assigned them for us by default\n",
    "- Think of them as row labels (and yep, they are customizable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize indices to your liking\n",
    "pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns=[\"A\",\"B\",\"C\"], index=[\"x\",\"y\",\"z\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Describing Dataframes\n",
    "\n",
    "Let's look at common functions to examine our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the dimensions of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the # of elements\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the first or last N rows\n",
    "df.head() # or df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To sample the dataset for previewing\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show a summary of the composition of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show some basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the unique # of elements\n",
    "df.nunique()\n",
    "\n",
    "# To apply for a specific column\n",
    "# df['A'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the indices in an array\n",
    "df.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. From Files to DataFrames\n",
    "\n",
    "- A more common use case in the real world however would be to load data from files\n",
    "- Pandas supports tons of file formats which can be loaded easily through the `.read_*` function\n",
    "- Using the same exact __Olympics Results__ dataset in different file formats, let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "results = pd.read_csv('./data/olympics_results.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel file\n",
    "results = pd.read_excel('./data/olympics_data.xlsx')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It loaded the default sheet, but we can specify a specific sheet\n",
    "results = pd.read_excel('./data/olympics_data.xlsx', sheet_name=\"results\")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feather file\n",
    "result = pd.read_feather('./data/olympics_results.feather')\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file\n",
    "results = pd.read_parquet('./data/olympics_results.parquet')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge check ðŸ’¡\n",
    "\n",
    "_Notice any differences when dealing with various file formats using the same exact dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. From DataFrames to Files\n",
    "\n",
    "- We can also export data back to files with pandas\n",
    "- Useful for saving processed/cleaned data\n",
    "- This is done through the `.to_*` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into JSON file\n",
    "results.to_json('./results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Accessing Data\n",
    "\n",
    "- Selecting elements from a dataframes can commonly be done using Python's slice notation (\"`:`\")\n",
    "- Use `.iloc` for integer-based indexing and `.loc` for label-based indexing\n",
    "- Use `.iat` or `.at` for fast scalar value access (single cell)\n",
    "- Use these native indexer methods to efficiently select elements in dataframes whenever you can!\n",
    "- For these purposes, we'll be using our __toy dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv('./coffee.csv')\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select specific rows using index\n",
    "coffee.iloc[[0,1,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select rows from index 10 onwards\n",
    "coffee.iloc[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select rows using start and stop positions\n",
    "coffee.iloc[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge check ðŸ’¡\n",
    "\n",
    "_Pandas is actually following Python's zero-based indexing when slicing and it uses an exclusive upper bound. Why do you think this is done this way?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select all rows\n",
    "coffee.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select all rows with specific columns\n",
    "coffee.iloc[:, ['Day', 'Coffee Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc to the rescue\n",
    "coffee.loc[:, ['Day', 'Coffee Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using index labels instead of default integers\n",
    "coffee.index = coffee[\"Day\"]\n",
    "coffee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc works with labels\n",
    "coffee.loc[\"Monday\":\"Wednesday\", \"Units Sold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv('./coffee.csv') # reset df\n",
    "\n",
    "# Use .iat or .at for selecting specific cells\n",
    "coffee.iat[0,0]\n",
    "coffee.at[0,\"Day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select all rows using a specific column\n",
    "coffee.Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But square brackets are more robust because it supports both single and multiple worded columns\n",
    "coffee['Units Sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee.sort_values([\"Units Sold\", \"Coffee Type\"], ascending=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can manually iterate on the rows but USE SPARINGLY\n",
    "# Not using the builtin methods loses the memory and performance benefits of pandas\n",
    "for index, row in coffee.iterrows():\n",
    "    print(index)\n",
    "    print(row)\n",
    "    # print(row[\"Coffee Type\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Filtering Data\n",
    "\n",
    "- Accessing data requires precise selection whereas filtering allows us to describe certain __criteria__ to match elements\n",
    "- This is mostly done using __boolean indexing__ and __comparison__ operators\n",
    "- For this chapter, we'll use the __Olympics Biography__ dataset to explore filtering techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's our dataset\n",
    "bios = pd.read_csv('./data/olympics_bios.csv')\n",
    "bios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select rows from specific columns based on height condition\n",
    "df = bios.loc[bios['height_cm'] > 215, ['name', 'height_cm']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short-hand notation\n",
    "df = bios[bios['height_cm'] > 215][['name', 'height_cm']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter using multiple conditions\n",
    "bios[(bios['height_cm'] > 215)  & (bios['born_country'] == 'USA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter using string operations\n",
    "bios[bios['name'].str.contains(\"maron\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter names with repeated letters using regex\n",
    "repeated_letters = bios[bios['name'].str.contains(r'(.)\\1', na=False)]\n",
    "repeated_letters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter names with 'son' or 'sen' at the end using regex\n",
    "son_sen_names = bios[bios['name'].str.contains(r'son$|sen$', case=False, na=False)]\n",
    "son_sen_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To filter athletes from the 90's using regex\n",
    "born_90s = bios[bios['born_date'].str.contains(r'^199', na=False)] # ignore NaN values\n",
    "born_90s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can mix and match things!\n",
    "bios[bios['born_country'].isin([\"PHI\"]) & (bios['name'].str.startswith(\"Hidilyn\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Manipulating DataFrames\n",
    "\n",
    "- Adding, modifying or dropping columns may become necessary as we make our dataset cleaner and more robust\n",
    "- We can store derived values from existing columns into a new column (e.g market_cap = price * shares)\n",
    "- Irrelevant or redundant columns can be dropped\n",
    "- Let's see some examples in action using our toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new column with fixed values\n",
    "coffee['price'] = 100\n",
    "coffee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add the price with a smarter approach using numpy's conditional where\n",
    "import numpy as np\n",
    "\n",
    "coffee['new_price'] = np.where(coffee['Coffee Type']=='Espresso', 100, 150)\n",
    "coffee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to delete the previous column\n",
    "coffee.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait what-- the old price column is still here\n",
    "coffee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes are immutable by default, so drop() returned a new copy. We can set inplace to True to override this.\n",
    "coffee.drop(columns=['price'], inplace=True)\n",
    "\n",
    "# This is perfectly fine as well\n",
    "# coffee = coffee.drop(columns=['price'])\n",
    "\n",
    "# Alternatively...\n",
    "# coffee = coffee[['Day', 'Coffee Type', 'Units Sold', 'new_price']]\n",
    "\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new column and deriving its values based on existing columns\n",
    "coffee ['revenue'] = coffee['Units Sold'] * coffee['new_price']\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename a column\n",
    "coffee.rename(columns={'new_price': 'price'}, inplace=True)\n",
    "coffee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example: Let's store the first name as a new column\n",
    "bios['first_name'] = bios['name'].str.split(' ').str[0]\n",
    "bios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios.query('first_name == \"Juan\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new column and store values based on custom logic using python lambdas\n",
    "bios['height_category'] = bios['height_cm'].apply(lambda x: 'Short' if x < 165 else ('Average' if x <185 else 'Tall'))\n",
    "bios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To define and use regular functions instead of lambdas\n",
    "def categorize_athlete(row):\n",
    "    if row['height_cm'] < 175 and row['weight_kg'] < 70:\n",
    "        return 'Lightweight'\n",
    "    elif row ['height_cm'] < 185 or row['weight_kg'] <= 80:\n",
    "        return 'Middleweight'\n",
    "    \n",
    "    else:\n",
    "        return 'Heavyweight'\n",
    "\n",
    "bios['weight_category'] = bios.apply(categorize_athlete, axis=1) # 1 is rows, 0 is columns\n",
    "bios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII. Combining DataFrames\n",
    "\n",
    "#### Merging\n",
    "\n",
    "- In the real world, typical datasets are normalized and split into compact, purposeful tables\n",
    "  - For example, sales records for a coffee shop can be stored in one dataframe, while the menu can be in another\n",
    "- Depending on our use case, we'd want to combine various dataframes to derive meaningful insights\n",
    "- It is important to identify a column from a dataframe that **relates** to a column in another dataframe\n",
    "- But how do we exactly merge two datasets in pandas? This is done through `pd.merge()`.\n",
    "- Think of `pd.merge()` like combining two spreadsheets based on a common column, similar to matching information from two lists.\n",
    "\n",
    "##### Type of Joins in Pandas\n",
    "\n",
    "<img src=\"images/pandas_basic_joins.png\" alt=\"BMI Formula\" width=\"500\" />\n",
    "\n",
    "- **INNER JOIN:** Only keeps rows where the matching column exists in BOTH tables\n",
    "  - Like finding common friends between two people\n",
    "- **LEFT JOIN:** Keeps ALL rows from the left table, even if no match in right table\n",
    "  - Like keeping your full class list and adding grades where available\n",
    "- **RIGHT JOIN:** Keeps ALL rows from the right table, even if no match in left table-\n",
    "  - Like keeping all grades, even for transferred students not in current roster\n",
    "- **OUTER JOIN:** Keeps ALL rows from BOTH tables\n",
    "  - Like combining two class rosters completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe how \"NOC\" column relates to the \"born_country\" column in our Olympics Biography dataset\n",
    "nocs = pd.read_csv('./data/noc_regions.csv')\n",
    "nocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we specify the two dataframes\n",
    "# Then, we specify their respective columns that need to match values across rows\n",
    "# Finally, we specify the join type to apply for the merge\n",
    "# LEFT JOIN will keep all rows from \"bios\" even if there is no match in \"nocs\" (they will simply be NAs)\n",
    "bios_new = pd.merge(bios, nocs, left_on='born_country', right_on='NOC', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that pandas automatically resolves conflicting column names by adding \"_<suffix>\"\n",
    "# Making the column name more meaningful in the context of Olympics Biography\n",
    "bios_new.rename(columns={'region': 'born_country_full'}, inplace=True)\n",
    "bios_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this new information, we can gather more insights such as\n",
    "# Finding atheletes who competed under a different region\n",
    "bios_new[bios_new['NOC_x'] != bios_new['born_country_full']][['name','NOC_x','born_country_full']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation\n",
    "\n",
    "- Perfect for combining similar data structures\n",
    "- Like stacking blocks on top of each other (rows) or side by side (columns)\n",
    "- This is done through `pd.concat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To concatenate two filtered sub-dataframes from the same origin dataframe\n",
    "us_df = bios[bios['born_country'] == 'USA'].copy()\n",
    "ph_df = bios[bios['born_country'] == 'PHI'].copy()\n",
    "\n",
    "combined_df = pd.concat([us_df,ph_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge check ðŸ’¡\n",
    "\n",
    "_What do you think is the main difference between merging and concatenating dataframes?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX. Aggregating Data\n",
    "\n",
    "- Aggregating data enables us to extract insights and identify patterns crucial for data analysis\n",
    "- Use cases include:\n",
    "    - Grouping data by categories\n",
    "    - Identifying trends and distributions\n",
    "    - Performing statistical analysis (mean, median, standard deviation, etc)\n",
    "    - Providing business metrics (total sales, average revenue, etc)\n",
    "- In this chapter, let's revisit our __Olympics Biography__ and Toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To count unique values of a column\n",
    "bios['born_city'].value_counts() # budapest leading the charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine state that has the most # of athletes\n",
    "bios[bios['born_country'] == 'USA']['born_region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the total unit sold grouped by coffee type\n",
    "coffee.groupby(['Coffee Type'])['Units Sold'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get multiple aggregations with the same group by\n",
    "coffee.groupby(['Coffee Type']).agg({'Units Sold': 'sum', 'price': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get multiple aggregations with multiple group by\n",
    "coffee.groupby(['Coffee Type', 'Day']).agg({'Units Sold': 'sum', 'price': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try grouping by born_date\n",
    "bios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To group by born_date and get unique name count\n",
    "bios['born_date'] = pd.to_datetime(bios['born_date']) # onvert from string to proper pandas datetime format\n",
    "bios.groupby(bios['born_date'].dt.year)['name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reset the index with defaults\n",
    "bios.groupby(bios['born_date'].dt.year)['name'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To sort such that the highest count is at the top\n",
    "bios.groupby(bios['born_date'].dt.year)['name'].count().reset_index().sort_values('name', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X. Other Useful Operations\n",
    "\n",
    "There is a myriad of functionalities offered in pandas which you can find more about in their [official documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html#user-guide). However, here are some more useful functionalities that might come handy at your disposal. ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivoting\n",
    "\n",
    "- Powerful way to reshape your data by turning unique column values as new columns\n",
    "- Helps transform data from \"long\" to \"wide\" format\n",
    "- This can be achieved through `df.pivot`, but requires unique index/value combinations\n",
    "- Consider `pd.pivot_table` to reshape and aggregate duplicate values in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to our toy dataset\n",
    "coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the Coffee Type as columns and revenue as the rows\n",
    "pivoted = coffee.pivot(columns='Coffee Type', index='Day', values='revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted # ...how is this useful? Well, it allows for different perspective on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much sales we got from Latte on Monday?\n",
    "pivoted.loc['Monday', 'Latte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's our total sales per product?\n",
    "pivoted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's our total sales per business day?\n",
    "pivoted.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas DateTime\n",
    "\n",
    "- It's a good practice to represent date and datetime values in actual datetime objects\n",
    "- This allows for:\n",
    "  - More efficient operations\n",
    "  - Easy date arithmetic\n",
    "  - Consistent format handling\n",
    "  - Timezone handling\n",
    "  - Integration with timeseries tools from pandas and other libraries\n",
    "- Looking back at our __Olympics Biography__ dataset, we see that `born_date` is of type `object` (string)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to add a proper datetime column\n",
    "bios['born_datetime'] = pd.to_datetime(bios['born_date'])\n",
    "bios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While values appear similar, its type is now datetime64\n",
    "bios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas datetime unlocks a rich set of datetime utilities\n",
    "\n",
    "# Store the year\n",
    "bios['born_year'] = bios['born_datetime'].dt.year\n",
    "\n",
    "# Derive the athlete's age, null out if deceased\n",
    "bios['age'] = np.where(bios['died_date'].isna(), pd.Timestamp.now().year - bios['born_year'], np.nan)\n",
    "bios[['name','born_year', 'age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Null values\n",
    "\n",
    "It is common for certain datasets to have null values in some columns. Depending on the use case, you may choose to (1) ignore them, (2) fill them with a default value, or (3) drop these rows with null values.\n",
    "\n",
    "Let's examine our __Olympics Biography__ dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that non-null values are not always similar in count? That is because there's likely null values in some columns.\n",
    "bios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surely enough...\n",
    "bios.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN = Not a Number\n",
    "bios.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop rows with missing height and/or weight information\n",
    "bios.dropna(subset=['height_cm', 'weight_kg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fill missing height information with a default value, i.e median\n",
    "bios.fillna(bios['height_cm'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking Data\n",
    "\n",
    "Dataframes also allow us to easily compute for numerical ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking them by height and see what we got?\n",
    "bios['height_rank'] = bios['height_cm'].rank(ascending=False)\n",
    "bios.sort_values('height_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epilogue: Brainteaser ðŸ§ \n",
    "\n",
    "How about quick warm up exercises to enforce some concepts before we jump into the group work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Hidilyn Diaz's Olympic Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start wrangling here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the GOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start wrangling here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
